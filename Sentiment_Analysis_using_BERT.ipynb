{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMv/L5H9+ojlxO5O4vvAn8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandini-n-123/DAALEETCODE/blob/main/Sentiment_Analysis_using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ8JWX-9nPk_",
        "outputId": "20ea8df0-7cc1-44ec-828f-303c100a0d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.3782\n",
            "Epoch 2, Training Loss: 0.1424\n",
            "Epoch 3, Training Loss: 0.0661\n",
            "Validation Accuracy: 0.9062, Precision: 0.9063, Recall: 0.9062, F1-Score: 0.9062\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv').head(4000)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        " #Preprocess the labels into numerical form\n",
        "label_encoder = LabelEncoder()\n",
        "df['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])  # 0=negative, 1=neutral, 2=positive\n",
        "\n",
        "#splitting the datset into test and train datasets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['review'], df['sentiment_encoded'], test_size=0.2)\n",
        "\n",
        "#initializing the bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Tokenize and encode sequences\n",
        "def preprocess_texts(texts, tokenizer, max_length=512):\n",
        "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "train_encodings = preprocess_texts(train_texts, tokenizer)\n",
        "val_encodings = preprocess_texts(val_texts, tokenizer)\n",
        "\n",
        "# Create Torch Dataset\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels.values))\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels.values))\n",
        "\n",
        "# DataLoader for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(3):  # You can increase the number of epochs based on dataset size\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)  # Forward pass\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "    # Validation after each epoch\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# printing the accuracy and precision and recall f1_score of the model\n",
        "accuracy = accuracy_score(true_labels, preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted')\n",
        "print(f'Validation Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7CMlb03xxjbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "529VEsdLyNzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}